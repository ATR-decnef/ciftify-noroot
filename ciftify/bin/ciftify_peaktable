#!/usr/bin/env python
"""
Takes a cifti map ('dscalar.nii' and outputs a csv of peak vertices)

Usage:
    ciftify_peaktable [options] <func.dscalar.nii>

Arguments:
    <func.dscalar.nii>    Input map.

Options:
    --min-threshold MIN    the largest value [default: -2.85] to consider for being a minimum
    --max-threshold MAX    the smallest value [default: 2.85] to consider for being a maximum
    --area-threshold MIN   threshold [default: 20] for surface cluster area, in mm^2
    --outputcsv FILE       Filename of the output csv table
    --output-clusters      Output a dscalar map of the clusters
    --left-surface GII     Left surface file (default is HCP S900 Group Average)
    --right-surface GII    Right surface file (default is HCP S900 Group Average)
    --left-surf-area GII   Left surface vertex areas file (default is HCP S900 Group Average)
    --right-surf-area GII  Right surface vertex areas file (default is HCP S900 Group Average)
    --surface-distance MM  minimum distance in mm [default: 20] between extrema of the same type.
    --volume-distance MM   minimum distance in mm [default: 20] between extrema of the same type.
    --debug                Debug logging
    -n,--dry-run           Dry run
    -h, --help             Prints this message

DETAILS
Note: at the moment this only outputs the peaks from the surface component of the cifti map.
Use -cifti-separate in combination with FSL's clusterize to get the volume peaks

If no surfaces of surface area files are given. The midthickness surfaces from
the HCP S900 Group Mean will be used, as well as it's vertex-wise
surface area infomation.

Default name for the output csv taken from the input file.
i.e. func.dscalar.nii --> func_peaks.csv

If the '--output-clusters' flag is used, a dscalar.nii file will be written
to the same folder as the outputcsv file with similar naming, ending in '_clust.dscalar.nii'.
(i.e. func_peaks.csv & func_clust.dscalar.nii)

Written by Erin W Dickie, June 3, 2016
"""

from __future__ import division

import os
import numpy as np
import scipy as sp
import nibabel as nib
import pandas as pd
import ciftify
from docopt import docopt

import ciftify

config_path = os.path.join(os.path.dirname(__file__), "logging.conf")
logging.config.fileConfig(config_path, disable_existing_loggers=False)
logger = logging.getLogger(os.path.basename(__file__))

def docmd(cmdlist):
    "sends a command (inputed as a list) to the shell"
    logger.debug("Running command: {}".format(' '.join(cmdlist)))
    if not DRYRUN: subprocess.call(cmdlist)


def load_hemisphere_data(filename, wb_structure):
    '''loads data from one hemisphere of dscalar,nii file'''

    with ciftify.utilities.TempDir() as little_tempdir:
        ## separate the cifti file into left and right surfaces
        data_gii = os.path.join(little_tempdir, 'data.func.gii')
        docmd(['wb_command','-cifti-separate', filename, 'COLUMN',
            '-metric', wb_structure, data_gii])

        # loads label table as dict and data as numpy array
        data = ciftify.utilities.load_gii_data(data_gii)
    return data


def load_hemisphere_labels(filename, wb_structure, map_number = 1):
    '''separates dlabel file into left and right and loads label data'''

    with ciftify.utilities.TempDir() as little_tempdir:
        ## separate the cifti file into left and right surfaces
        labels_gii = os.path.join(little_tempdir, 'data.label.gii')
        docmd(['wb_command','-cifti-separate', filename, 'COLUMN',
            '-label', wb_structure, labels_gii])

        # loads label table as dict and data as numpy array
        gifti_img = nib.load(labels_gii)
        atlas_data = gifti_img.get_arrays_from_intent('NIFTI_INTENT_LABEL')[map_number - 1].data

        atlas_dict = gifti_img.labeltable.get_labels_as_dict()
        atlas_df = pd.DataFrame.from_dict(atlas_dict, orient = "index")

    return atlas_data, atlas_df


def wb_cifti_clusters(input_cifti, output_cifti, surf_settings,
                      value_threshold, minimun_size,
                      less_than, starting_label=1, surface_only = False):
    '''runs wb_command -cifti-find-clusters'''
    wb_arglist = ['wb_command', '-cifti-find-clusters',
            input_cifti,
            str(value_threshold), str(minimun_size),
            str(value_threshold), str(minimun_size),
            'COLUMN',
            output_cifti,
            '-left-surface', surf_settings['L']['surface'],
            '-corrected-areas', surf_settings['L']['vertex_areas'],
            '-right-surface', surf_settings['R']['surface'],
            '-corrected-areas', surf_settings['R']['vertex_areas'],
            '-start', str(starting_label)]
    if less_than : wb_arglist.append('-less-than')
    if not surface_only : wb_arglist.append('-merged-volume')
    docmd(wb_arglist)


def calc_cluster_areas(df, clust_labs, surf_va):
    '''
    calculates the surface area column of the peaks table
    needs hemisphere specific inputs
    '''
    for cID in df.clusterID.unique():
        idx = np.where(clust_labs == cID)[0]
        area = sum(surf_va[idx])
        df.loc[df.loc[:,'clusterID']==cID,'area'] = area
    return(df)



def calc_atlas_overlap(df, wb_structure, clust_label_array, surf_va, atlas_settings):
    '''
    calculates the surface area column of the peaks table
    needs hemisphere specific inputs
    '''

    ## load atlas
    atlas_label_array, atlas_df = load_hemisphere_labels(atlas_settings['path'],
                                                       wb_structure,
                                                       map_number = atlas_settings['map_number'])
    atlas_prefix = atlas_settings['name']

    ## create new cols to hold the data
    df[atlas_prefix] = pd.Series('not_calculated', index = df.index)
    overlap_col = '{}_overlap'.format(atlas_prefix)
    df[overlap_col] = pd.Series(-99.0, index = df.index)

    for pd_idx in df.index.tolist():
        ## atlas interger label is the integer at the vertex
        atlas_label = atlas_label_array[df.loc[pd_idx, 'vertex']]

        ## the atlas column holds the labelname for this label
        df.loc[pd_idx, atlas_prefix] = atlas_df.iloc[atlas_label, 0]

        ## overlap indices are the intersection of the cluster and the atlas integer masks
        clust_mask = np.where(clust_label_array == df.loc[pd_idx, 'clusterID'])[0]
        atlas_mask = np.where(atlas_label_array == atlas_label)[0]
        overlap_mask = np.intersect1d(clust_mask,atlas_mask)

        ## overlap area is the area of the overlaping region over the total cluster area
        clust_area = df.loc[pd_idx, 'area']
        overlap_area = sum(surf_va[overlap_mask])
        df.loc[pd_idx, overlap_col] = overlap_area/clust_area

    return(df)


def build_hemi_results_df(surf_settings, atlas_settings,
                          input_dscalar, extreama_dscalar, cluster_dscalar):

    ## read in the extrema file from above
    extrema_array = load_hemisphere_data(extreama_dscalar, surf_settings['wb_structure'])
    vertices = np.nonzero(extrema_array)[0]  # indices - vertex id for peaks in hemisphere

    ## read in the original data for the value column
    input_data_array = load_hemisphere_data(input_dscalar, surf_settings['wb_structure'])

    ## load both cluster indices
    clust_array = load_hemisphere_data(clusters_dscalar, surf_settings['wb_structure'])

    ## load the coordinates
    coords = nib.load(surf_settings['surface']).get_arrays_from_intent('NIFTI_INTENT_POINTSET')[0].data
    surf_va = ciftify.utilities.load_gii_data(surf_settings['vertex_areas'])

    ## put all this info together into one pandas dataframe
    df = pd.DataFrame({"clusterID": np.reshape(extrema_array[vertices],(len(vertices),)),
                    "hemisphere": surf_settings['hemi'],
                    "vertex": vertices,
                    'x': np.round(coords[vertices,0]),
                    'y': np.round(coords[vertices,1]),
                    'z': np.round(coords[vertices,2]),
                    'value': np.reshape(input_data_array[vertices],(len(vertices),)),
                    'area': -99.0})

    ## calculate the area of the clusters
    df = calc_cluster_areas(df, clust_array, surf_va)

    ## look at atlas overlap
    for atlas in atlas_settings.keys():
        df = calc_atlas_overlap(df, surf_settings['wb_structure'], clust_array, surf_va, atlas_settings[atlas])

    return(df)

def define_atlas_settings():
    atlas_settings = {
        'DKT': {
            'path' : os.path.join(ciftify.config.find_HCP_S1200_GroupAvg(),
                                  'cvs_avg35_inMNI152.aparc.32k_fs_LR.dlabel.nii'),
            'order' : 1,
            'name': 'DKT',
            'map_number': 1
        },

        'Yeo7': {
            'path' : os.path.join(ciftify.config.find_HCP_S1200_GroupAvg(),
                                  'RSN-networks.32k_fs_LR.dlabel.nii'),
            'order' : 2,
            'name' : 'Yeo7',
            'map_number': 1
        },
        'MMPI': {
            'path': os.path.join(ciftify.config.find_HCP_S1200_GroupAvg(),
                                 'Q1-Q6_RelatedValidation210.CorticalAreas_dil_Final_Final_Areas_Group_Colors.32k_fs_LR.dlabel.nii'),
            'order' : 3,
            'name' : 'MMPI',
            'map_number': 1
        }
    }
    return(atlas_settings)

def define_surface_settings():
    surf_settings = {
        'L': {
            'surface' : os.path.join(ciftify.config.find_HCP_S1200_GroupAvg(),
                                    'S1200.L.midthickness_MSMAll.32k_fs_LR.surf.gii'),
            'vertex_areas' : os.path.join(ciftify.config.find_HCP_S1200_GroupAvg(),
                                    'S1200.L.midthickness_MSMAll_va.32k_fs_LR.shape.gii'),
            'wb_structure' : 'CORTEX_LEFT',
            'hemi' : 'L'
        },
        'R': {
            'surface' : os.path.join(ciftify.config.find_HCP_S1200_GroupAvg(),
                                    'S1200.R.midthickness_MSMAll.32k_fs_LR.surf.gii'),
            'vertex_areas' : os.path.join(ciftify.config.find_HCP_S1200_GroupAvg(),
                                    'S1200.R.midthickness_MSMAll_va.32k_fs_LR.shape.gii'),
            'wb_structure' : 'CORTEX_RIGHT',
            'hemi' : 'R'
        },
    }
    return(surf_settings)


def main(tmpdir):
    global DRYRUN

    arguments = docopt(__doc__)
    data_file = arguments['<func.dscalar.nii>']
    surfL = arguments['--left-surface']
    surfR = arguments['--right-surface']
    surfL_va = arguments['--left-surf-area']
    surfR_va = arguments['--right-surf-area']
    surf_distance = arguments['--surface-distance']
    volume_distance = arguments['--volume-distance']
    min_threshold = arguments['--min-threshold']
    max_threshold = arguments['--max-threshold']
    area_threshold = arguments['--area-threshold']
    outputbase = arguments['--outputcsv']
    output_clusters = ['--output-clusters']
    debug = arguments['--debug']
    DRYRUN = arguments['--dry-run']

    if debug:
        logger.setLevel(logging.DEBUG)
        logging.getLogger('ciftify').setLevel(logging.DEBUG)


    atlas_settings = define_atlas_settings()
    ## if not outputname is given, create it from the input dscalar map
    if not outputbase:
        outputbase = data_file.replace('.dscalar.nii','')

    outputcsv_cortex = '{}_cortex.csv'.format(outputbase)
    outputcsv_sub = '{}_subcortical.csv'.format(outputbase)

    if output_clusters:
        clusters_dscalar = '{}_clust.dscalar.nii'.format(outputbase)
    else:
        clusters_dscalar = os.path.join(tmpdir,'clusters.dscalar.nii')

    ## grab surface files from the HCP group average if they are not specified
    surf_settings = define_surface_settings()
    use_HCP_S900 = True
    if surfL: surf_settings['L']['surface'] = surfL
    if surfR:
        surf_settings['R']['surface'] = surfR
        use_HCP_S1200 = False

    ## get surfL_va from HCP average...or calculate them from the surface
    if not surfL_va:
        surf_settings['L']['surface'] = surfL_va
        surfL_va = os.path.join(tmpdir, 'surfL_va.shape.gii')
        surfR_va = os.path.join(tmpdir, 'surfR_va.shape.gii')
        if use_HCP_S900:
            docmd(['wb_command', '-cifti-separate',
                os.path.join(ciftify.config.find_HCP_S900_GroupAvg(),
                'S900.midthickness_MSMAll_va.32k_fs_LR.dscalar.nii'),
                'COLUMN',
                '-metric', 'CORTEX_LEFT', surfL_va,
                '-metric', 'CORTEX_RIGHT', surfR_va])
        else:
            docmd(['wb_command', '-surface-vertex-area', surfL, surfL_va])
            docmd(['wb_command', '-surface-vertex-area', surfR, surfR_va])

    ## run wb_command -cifti-extrema to find the peak locations
    extrema_dscalar = os.path.join(tmpdir,'extrema.dscalar.nii')
    docmd(['wb_command','-cifti-extrema',
           data_file,
           str(surf_distance), str(volume_distance),'COLUMN',
           extrema_dscalar,
           '-left-surface', surf_settings['L']['surface'],
           '-right-surface', surf_settings['R']['surface'],
           '-threshold', str(min_threshold), str(max_threshold)])

    ## also run clusterize with the same settings to get clusters
    pcluster_dscalar = os.path.join(tmpdir,'pclusters.dscalar.nii')

    wb_cifti_clusters(data_file, pcluster_dscalar, surf_settings,
                      max_threshold, area_threshold,
                      less_than = False, starting_label=1, surface_only = surface_only)

    ## load both cluster files to determine the max value
    pos_clust_data = ciftify.utilities.load_surfaceonly(pcluster_dscalar)
    max_pos = int(np.max(pos_clust_data))

    ## now get the negative clusters
    ncluster_dscalar = os.path.join(tmpdir,'nclusters.dscalar.nii')
    wb_cifti_clusters(data_file, ncluster_dscalar, surf_settings,
                      min_threshold, area_threshold,
                      less_than = True, starting_label=max_pos + 1, surface_only = surface_only)

    ## add the positive and negative together to make one cluster map

    docmd(['wb_command', '-cifti-math', '(x+y)',
        clusters_dscalar,
        '-var','x',pcluster_dscalar, '-var','y',ncluster_dscalar])

    ## multiply the cluster labels by the extrema to get the labeled exteama
    lab_extrema_dscalar = os.path.join(tmpdir,'lab_extrema.dscalar.nii')
    docmd(['wb_command', '-cifti-math', '(x*y)',
        lab_extrema_dscalar,
        '-var','x',clusters_dscalar, '-var','y',extrema_dscalar])

    ## run left and right dfs... then concatenate them
    dfL = build_hemi_results_df(surf_settings['L'], atlas_settings,
                              data_file, lab_extrema_dscalar, clusters_dscalar)
    dfR = build_hemi_results_df(surf_settings['R'], atlas_settings,
                              data_file, lab_extrema_dscalar, clusters_dscalar)
    df = dfL.append(dfR, ignore_index = True)

    ## write the table out to the outputcsv
    output_columns = ['clusterID','hemisphere','vertex','x','y','z', 'value', 'area']
    for atlas in atlas_settings.keys():
        output_columns.append(atlas_settings[atlas]['name'])
        output_columns.append('{}_overlap'.format(atlas_settings[atlas]['name']))
    df.to_csv(outputcsv_cortex,
          columns = output_columns,index=False)

    ## write the table out to the outputcsv
    df.to_csv(outputcsv_cortex,
          columns = ['clusterID','hemisphere','vertex','x','y','z', 'value', 'area'],
          index=False)

    ## run FSL's cluster on the subcortical bits
    ## now to run FSL's cluster on the subcortical bits
    subcortical_vol = os.path.join(tmpdir, 'subcortical.nii.gz')
    docmd(['wb_command', '-cifti-separate', data_file, 'COLUMN', '-volume-all', subcortical_vol])
    fslcluster_cmd = ['cluster',
        '--in={}'.format(subcortical_vol),
        '--thresh={}'.format(max_threshold),
        '--peakdist={}'.format(volume_distance)]
    peak_table = subprocess.check_output(fslcluster_cmd)
    with open(outputcsv_sub, "w") as text_file:
        text_file.write(peak_table.replace('/t',','))


if __name__ == '__main__':
    with ciftify.utilities.TempDir() as tmpdir:
        logger.info('Creating tempdir:{} on host:{}'.format(tmpdir,
                    os.uname()[1]))
        ret = main(tmpdir)
    sys.exit(ret)
